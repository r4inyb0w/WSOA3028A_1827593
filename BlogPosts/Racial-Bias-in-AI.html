<!DOCTYPE html>
<html lang="en">

<head>
    <link href="../CSS/stylesheet.css" rel="stylesheet">
    <script src="/WSOA3028A_1827593/JS/navBar.js" async></script>
    <meta charset="utf-8">
    <meta name="description" content="Blog Post: Racial Bias in AI">
    <meta name="author" content="Samantha Thurgood 1827593">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">

    <meta property="og:title" content="Macey Daniels: My Blogs" />
    <meta property="og:type" content="website" />
    <meta property="og:url" content="https://r4inyb0w.github.io/WSOA3028A_1827593/BlogPosts/Racial-Bias-in-AI.html" />
    <meta property="og:image" content="https://r4inyb0w.github.io/WSOA3028A_1827593/Images/ogpPic.png" />
    <meta property="og:description" content="Racial Bias in AI" />

    <title>
        Racial Bias in AI
    </title>
</head>

<body onload='createNavBar()'>
    <nav></nav>

    <h1 style="text-align: center;">
        <p>Blog Posts</p>
    </h1>
    <section style="padding-left: 18%; padding-right: 45%; text-align: justify;">
        <h2>
            Racial Bias in AI
        </h2>
        <time datetime=" 2020/05/22">Friday, 22 May</time>
        <article>
            <p>Artificial intelligence (AI) is biased. This is due to the training it undergoes and is based on those
                who develop it. People have different perspectives and experiences and as such cannot develop systems
                that are completely unbiased. These developers choose their training material based on their own
                experience. So, if a system is trained using only white faces, of course this AI will be biased towards
                white faces. However, this is a choice made by the developers and I believe it can be corrected. If
                developers take a larger, more diverse training dataset this can assist in moving towards a more
                unbiased technology. </p>

            <p>This is not happening: these developers have their own racial biases, the people paying for this
                technology to be built have racial biases, they do not have the time or resources to collect this many
                samples for training… which again is just racially biased. Whether or not these biases are intentional,
                they are present. Developers must actively work against these biases; they should recognize them and try
                fix them. Being oblivious cannot be an excuse. Something I have heard a lot recently is fitting for this
                argument: it is not enough to be non-racist; we must be anti-racist.</p>

            <p>I listened to The Radical AI Podcast: Racism and Sexism in AI Technology (Radical AI, 2020). Dr Sarah
                Myers West, who is a researcher at the AI Now Institute, explains that AI are trained with large
                datasets and use these datasets to make predictions about future patterns, however, these datasets are a
                reflection of social conditions and as such “amplify existing social inequalities”. However, to
                paraphrase Myers, we cannot base the problem and solution solely in technology. Meaning, we cannot rely
                on an unbiased system to solve societal inequalities, although I believe it is still a step in the right
                direction. </p>

            <p>This podcast further explains that trying to fix this problem could actually increase these social
                problems. For example, improved surveillance technology used for deportation is not helpful for people
                of colour who are now at a higher risk of deportation (Radical AI, 2020). So, this is another thing to
                consider: what is this technology being used for and is improving this system going to help or hinder
                people. As I mentioned earlier, this podcast emphasizes that discrimination in these systems is based on
                the creators and as such it is everybody’s responsibility to be anti-racist. Myers suggests, to improve
                these datasets, resources, such as payment, must be made available for people to engage. Additionally,
                external accountability – such as protests or regulations – should be used to ensure creators meet these
                diverse dataset requirements.</p>

            <p>Racial bias in technology often discusses the use of this technology as a danger to people of colour. For
                example, in China, there is a system named the “Integrated Joint Operations Platform” which is used to
                identify “suspicious” people (The Associated Press, 2019). This technology specifically targets Uighurs,
                who, once targeted, are either put on house arrest or sent to detention centers or prison (The
                Associated Press, 2019). It seems that China is not only targeting Uighurs in China but those who fled
                to other parts of the world as well. Mamattursun Omar is a Uighur chief. He was detained after returning
                from Egypt and tortured into naming other Uighurs who worked with him in Egypt (The Associated Press,
                2019).</p>

            <p>Furthermore, racially bias AI is present in the most mundane things. For example, a black woman in
                Britain was told her passport photo violated their rules since it claimed that her mouth was open,
                however, she just had big lips. The following was tweeted by this woman: “British passport system
                violated man said my mouth is open the digital system can’t process my these lips” (@elainebabey). See
                this tweet <a class="main" href="https://twitter.com/elainebabey/status/1232333491607625728 ">here</a>.
                One
                response in this Twitter thread from another woman of colour said: “mine said
                the background was black, it was my afro” (@sophieodira). See this tweet <a class="main"
                    href="https://twitter.com/sophieodira/status/1232365662389112832 ">here</a>. This is a clear
                example
                of AI technology not being well trained with a diverse dataset and as such the technology is biased
                against, in this specific example, black women. </p>

            <p>
                Racial bias in facial recognition technology is not a new thing. One similar instance of people of color
                being excluded from technical design is the camera film. Initially, the camera film was designed for
                what was known as the Shirley Tone: white women in coloured clothes (Vox, 2015). The chemicals that were
                used to develop colour did not produce darker red tones (Vox, 2015). Therefore, only light skin was
                captured well. There is a clear difference in the quality of images produced with different skin tones.
                Furniture and chocolate industries (Vox, 2015) were the first complaints to be heard and so they
                developed a camera system with two chips that allowed for lighter and darker skin tones to be produced
                evenly (Vox, 2015). This <a class="main"
                    href="https://www.youtube.com/watch?v=d16LNHIEJzs&feature=youtu.be">video</a> explains this well,
                the speaker emphasizes that there is “cultural bias
                in how we use technology and the technology itself” (Vox, 2015).
            </p>

            <p>
                I will leave you with this statement from Charlton McIlwain, from his <a class="main"
                    href="https://www.youtube.com/watch?v=KCefAIO_AD4&feature=youtu.be">video</a> discussing black
                software
                “these
                tools were not built for us… they were built to destroy us” (McIlwain, 2020).
            </p>

            <h3 style="text-align: center;">
                References
            </h3>
            <p>@elainebabey (2020). British passport system violated man said my mouth is open the digital system
                can’t process my these lips, Twitter. Available at:
                <cite>https://twitter.com/elainebabey/status/1232333491607625728</cite> (Accessed: 24 May 2020).</p>
            <p>@sophieodira (2020). mine said the background was black, it was my afro, Twitter. Available at:
                <cite>https://twitter.com/sophieodira/status/1232365662389112832</cite> (Accessed: 24 May 2020).
            </p>
            <p>Radical AI. (2020). The Radical AI Podcast: Racism and Sexism in AI Technology? Navigating Systems of
                Power with Sarah Myers West. Available at:
                <cite>https://open.spotify.com/episode/3RjZbORCXRGIYHoXOyCS1t?si=ZOGuKtx5RYywrwmob0Iu6Q</cite>.
                (Accessed: 25 May
                2020).</p>
            <p>Vox. (2015). Color film was built for white people. Here’s what it did to dark skin. YouTube. Available
                at:<cite> https://www.youtube.com/watch?v=d16LNHIEJzs&feature=youtu.be</cite>. (Accessed: 24 May 2020).
            </p>
            <p>McIlwain, C. (2020). The Internet & Racial Justice. YouTube. Available at:
                <cite>https://www.youtube.com/watch?v=KCefAIO_AD4&feature=youtu.be</cite>. (Accessed: 24 May 2020).</p>
            <p>
                The Associated Press. (2019). Secret documents reveal how China mass detention camps work. Available
                at: <cite>https://apnews.com/4ab0b341a4ec4e648423f2ec47ea5c47</cite>. (Accessed: 30 May 2020).
            </p>
        </article>
    </section>
    <p>
        <a class="main" style="padding-left: 18%;" href="A-little-example-of-the-digital-divide.html">Prev</a>
    </p>
    <footer>
        <section class="h-card">
            <p style="display: inline; padding-right: 20px;" class="p-tel">Contact Number: 011 450 8221 </p>
            <p style="display: inline; padding-right: 20px;" class="p-post-office-box"> P.O. Box 1212, Highway Gardens,
                Edenvale </p>
            <p style="display: inline;" class="p-street-address"> Daniels' Daisies Address: 98 1st Rd, Dunvegan,
                Johannesburg, 2090</p>
            <p>Latest Update: 8 November 1990</p>
        </section>
    </footer>
</body>

</html>